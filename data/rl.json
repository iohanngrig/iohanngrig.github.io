[
  {
    "title": "RL1: Markov Decision Processes",
    "description": "We discuss reinforcement learning in the context of fully observable Markov decision processes, where the information about the world is fully known, and the agent's goal is to find an optimal policy that maximizes the value function. We outline and implement policy and value iteration algorithms that find such an optimal policy and corresponding value function. We use these algorithms on the examples of \"FrozenLake\" and \"Taxi\" OpenAI Gym environments.",
    "url": "RL1_Markov_Decision_Processes.html",
    "media": "data/lake_opt.gif"
  },

  {
    "title": "RL2: Model-Free Policy Evaluation and Control",
    "description": "We discuss policy evaluation and control in situations where the dynamics and reward models of the world are unknown. We start with Monte Carlo methods that learn directly from sampling completed episodes of experience. Next, we outline Temporal Difference methods that can learn from incomplete episodes of experience. In the context of model-free control, we study Q-learning, SARSA, and expected SARSA models. We use various gym environments to explore the properties and efficiency of these methods.",
    "url": "RL2_Model_Free_Policy_Evaluation_and_Control.html",
    "media": "data/cartpole.gif"
  },

  {
    "title": "RL3: Value Function Approximation",
    "description": "Abstract.",
    "url": "RL3_Value_Function_Approximation.html",
    "media": "https://placehold.it/700x300"
  }

]
